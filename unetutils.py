import os
import cv2
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from io import BytesIO
import segmentation_models_pytorch as smp
from skimage.feature import local_binary_pattern

from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip

def create_and_load_unet_model(checkpoint_path, device, encoder_name='resnet34', num_classes=2):
    """
    T·∫°o m√¥ h√¨nh UNet++ v√† load tr·ªçng s·ªë t·ª´ checkpoint n·∫øu c√≥.
    """
    model = smp.UnetPlusPlus(
        encoder_name=encoder_name,
        encoder_weights="imagenet",
        in_channels=3,
        classes=num_classes
    ).to(device)

    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"‚úÖ Loaded model weights from {checkpoint_path}")
    else:
        print("‚ÑπÔ∏è No checkpoint found, using fresh model weights")

    model.eval()
    return model


def get_predicted_mask(model, image, transform, device):
    """
    Tr·∫£ v·ªÅ mask d·ª± ƒëo√°n ƒë√£ resize v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh g·ªëc.
    """
    original_size = image.size  # (width, height)
    image_transformed = transform(image).unsqueeze(0).to(device)

    model.eval()
    with torch.no_grad():
        output = model(image_transformed)
        pred_mask = torch.argmax(output, dim=1)

        pred_mask_resized = F.interpolate(
            pred_mask.unsqueeze(1).float(),
            size=(original_size[1], original_size[0]),
            mode='nearest'
        ).squeeze().cpu().numpy()

    return pred_mask_resized

def merge_and_expand_mask(mask, expand_px=20, connect_dist=5):
    """
    H·∫≠u x·ª≠ l√Ω mask: m·ªü r·ªông v√† n·ªëi c√°c v√πng g·∫ßn nhau.

    Parameters:
        mask (np.ndarray): Mask ƒë·∫ßu v√†o (gi√° tr·ªã 0 ho·∫∑c 1, dtype=np.uint8).
        expand_px (int): S·ªë pixel m·ªü r·ªông c√°c v√πng mask.
        connect_dist (int): Kho·∫£ng c√°ch t·ªëi ƒëa gi·ªØa c√°c v√πng ƒë·ªÉ n·ªëi l·∫°i.

    Returns:
        np.ndarray: Mask nh·ªã ph√¢n sau khi x·ª≠ l√Ω (gi√° tr·ªã 0 ho·∫∑c 255).
    """
    # ƒê·∫£m b·∫£o mask l√† nh·ªã ph√¢n uint8: 0 ho·∫∑c 255
    bin_mask = (mask > 0).astype(np.uint8) * 255

    # B∆∞·ªõc 1: Dilation ƒë·ªÉ m·ªü r·ªông v√πng mask
    kernel_expand = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (expand_px * 2 + 1, expand_px * 2 + 1))
    dilated_mask = cv2.dilate(bin_mask, kernel_expand, iterations=1)

    # B∆∞·ªõc 2: Closing ƒë·ªÉ n·ªëi c√°c v√πng g·∫ßn nhau (morphological closing = dilation + erosion)
    kernel_connect = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (connect_dist * 2 + 1, connect_dist * 2 + 1))
    connected_mask = cv2.morphologyEx(dilated_mask, cv2.MORPH_CLOSE, kernel_connect)

    return connected_mask

def get_bounding_boxes_from_mask(mask, min_area=100, offset=50):
    """
    Tr√≠ch xu·∫•t bounding boxes t·ª´ mask nh·ªã ph√¢n (pixel = 1).
    Offset m·ªói box th√™m `offset` pixel v·ªÅ m·ªói ph√≠a (v√† gi·ªõi h·∫°n trong k√≠ch th∆∞·ªõc ·∫£nh).
    """
    mask_uint8 = (mask * 255).astype(np.uint8)
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    height, width = mask.shape
    boxes = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if w > 0 and h > 0 and area >= min_area:
            x1 = max(x - offset, 0)
            y1 = max(y - offset, 0)
            x2 = min(x + w + offset, width - 1)
            y2 = min(y + h + offset, height - 1)
            boxes.append((x1, y1, x2, y2))

    return boxes



def visualize_prediction_with_boxes(model, image, transform, device):
    """
    D·ª± ƒëo√°n mask, v·∫Ω bounding box ch·ªìng l√™n ·∫£nh g·ªëc v√† hi·ªÉn th·ªã b·∫±ng matplotlib.
    """
    pred_mask_resized = get_predicted_mask(model, image, transform, device)
    boxes = get_bounding_boxes_from_mask(pred_mask_resized)
    # boxes = merge_overlapping_boxes(boxes, threshold=10)
    boxes = boxes

    plt.figure(figsize=(8, 8))
    plt.imshow(image)

    for box in boxes:
        x1, y1, x2, y2 = box
        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,
                             linewidth=2, edgecolor='lime', facecolor='none')
        plt.gca().add_patch(rect)

    plt.imshow(pred_mask_resized, cmap='jet', alpha=0.4)
    plt.title("D·ª± ƒëo√°n + Bounding Boxes")
    plt.axis('off')
    plt.tight_layout()
    plt.show()


def test_single_image(model, image, transform, device):
    """
    Hi·ªÉn th·ªã mask d·ª± ƒëo√°n ch·ªìng l√™n ·∫£nh g·ªëc.
    """
    original_size = image.size  # (width, height)
    image_tensor = transform(image).unsqueeze(0).to(device)

    model.eval()
    with torch.no_grad():
        output = model(image_tensor)
        pred_mask = torch.argmax(output, dim=1)

        pred_mask_resized = F.interpolate(
            pred_mask.unsqueeze(1).float(),
            size=(original_size[1], original_size[0]),
            mode='nearest'
        ).squeeze().cpu().numpy()

    plt.figure(figsize=(6, 6))
    plt.imshow(image)
    plt.imshow(pred_mask_resized, cmap='jet', alpha=0.5)
    plt.title("·∫¢nh g·ªëc + Mask d·ª± ƒëo√°n")
    plt.axis('off')
    plt.tight_layout()
    plt.show()

def test_single_image_streamlit(model, image, transform, device):
    """
    Tr·∫£ v·ªÅ ·∫£nh mask ch·ªìng l√™n ·∫£nh g·ªëc d∆∞·ªõi d·∫°ng PIL.Image ƒë·ªÉ hi·ªÉn th·ªã b·∫±ng st.image().
    """
    original_size = image.size  # (width, height)
    image_tensor = transform(image).unsqueeze(0).to(device)

    model.eval()
    with torch.no_grad():
        output = model(image_tensor)
        pred_mask = torch.argmax(output, dim=1)

        # Resize mask v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
        pred_mask_resized = F.interpolate(
            pred_mask.unsqueeze(1).float(),
            size=(original_size[1], original_size[0]),
            mode='nearest'
        ).squeeze().cpu().numpy()

    # T·∫°o ·∫£nh ch·ªìng mask b·∫±ng matplotlib
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.imshow(image)
    ax.imshow(pred_mask_resized, cmap='jet', alpha=0.5)
    ax.axis('off')

    # L∆∞u v√†o b·ªô nh·ªõ ƒë·ªám
    buf = BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
    plt.close(fig)
    buf.seek(0)
    
    # ƒê·ªçc l·∫°i b·∫±ng PIL
    result_image = Image.open(buf)
    return result_image


def boxes_overlap(box1, box2, threshold=10):
    """
    Ki·ªÉm tra 2 box c√≥ overlap ho·∫∑c c√°ch nhau kh√¥ng qu√° threshold pixel.
    Box d·∫°ng (x1, y1, x2, y2)
    """
    x11, y11, x12, y12 = box1
    x21, y21, x22, y22 = box2

    # M·ªü r·ªông box1 th√™m threshold
    x11_ext = x11 - threshold
    y11_ext = y11 - threshold
    x12_ext = x12 + threshold
    y12_ext = y12 + threshold

    # Ki·ªÉm tra c√≥ overlap (ho·∫∑c g·∫ßn nhau trong threshold) kh√¥ng
    if x12_ext < x21 or x22 < x11_ext:
        return False
    if y12_ext < y21 or y22 < y11_ext:
        return False
    return True


def merge_two_boxes(box1, box2, expand_pixels=50):
    """
    Tr·∫£ v·ªÅ bounding box bao ngo√†i c√πng c·ªßa 2 box,
    ƒë·ªìng th·ªùi m·ªü r·ªông th√™m expand_pixels pixel ·ªü m·ªói c·∫°nh.
    """
    x11, y11, x12, y12 = box1
    x21, y21, x22, y22 = box2

    x1 = min(x11, x21)
    y1 = min(y11, y21)
    x2 = max(x12, x22)
    y2 = max(y12, y22)

    # M·ªü r·ªông th√™m expand_pixels m·ªói c·∫°nh (gi·ªØ x1,y1 kh√¥ng √¢m)
    x1_exp = max(0, x1 - expand_pixels)
    y1_exp = max(0, y1 - expand_pixels)
    x2_exp = x2 + expand_pixels
    y2_exp = y2 + expand_pixels

    return (x1_exp, y1_exp, x2_exp, y2_exp)


def merge_overlapping_boxes(boxes, threshold=50, expand_pixels=0, min_area=500):
    """
    G·ªôp c√°c bounding boxes ch·ªìng l·∫•n ho·∫∑c g·∫ßn nhau th√†nh 1 box duy nh·∫•t,
    ƒë·ªìng th·ªùi m·ªü r·ªông m·ªói box th√™m expand_pixels pixel m·ªói c·∫°nh.
    Lo·∫°i b·ªè box c√≥ di·ªán t√≠ch nh·ªè h∆°n min_area.
    """
    merged = []

    for box in boxes:
        has_merged = False
        for i, mbox in enumerate(merged):
            if boxes_overlap(box, mbox, threshold):
                merged[i] = merge_two_boxes(box, mbox, expand_pixels)
                has_merged = True
                break
        if not has_merged:
            merged.append(box)

    # L·∫∑p l·∫°i ƒë·ªÉ h·ª£p nh·∫•t t·∫•t c·∫£ box c√≥ th·ªÉ ch·∫°m ho·∫∑c g·∫ßn nhau
    changed = True
    while changed:
        changed = False
        new_merged = []
        while merged:
            box = merged.pop(0)
            i = 0
            while i < len(merged):
                if boxes_overlap(box, merged[i], threshold):
                    box = merge_two_boxes(box, merged[i], expand_pixels)
                    merged.pop(i)
                    changed = True
                else:
                    i += 1
            new_merged.append(box)
        merged = new_merged

    # L·ªçc lo·∫°i b·ªè box c√≥ di·ªán t√≠ch < min_area
    filtered = []
    for box in merged:
        x1, y1, x2, y2 = box
        width = x2 - x1
        height = y2 - y1
        area = width * height
        if area >= min_area:
            filtered.append(box)

    return filtered



def get_boxes_from_prediction_with_merge(model, image, transform, device):
    """
    Tr·∫£ v·ªÅ danh s√°ch bounding boxes ƒë√£ ƒë∆∞·ª£c merge v√† mask d·ª± ƒëo√°n.
    """
    mask = get_predicted_mask(model, image, transform, device)
    boxes = get_bounding_boxes_from_mask(mask,900,20)
    merged_boxes = merge_overlapping_boxes(boxes, threshold=0, expand_pixels=0, min_area=1000)
    return merged_boxes, mask



def get_boxes_from_prediction(model, image, transform, device):
    """
    Tr·∫£ v·ªÅ danh s√°ch bounding boxes ƒë√£ ƒë∆∞·ª£c merge v√† mask d·ª± ƒëo√°n.
    """
    mask = get_predicted_mask(model, image, transform, device)
    boxes = get_bounding_boxes_from_mask(mask,900,0)
    return boxes, mask
    
import cv2
from PIL import Image
import numpy as np
import torch.nn.functional as F
from unetutils import create_and_load_unet_model, get_predicted_mask

import cv2
import os
import numpy as np
from PIL import Image
from torchvision import transforms
from unetutils import create_and_load_unet_model, get_predicted_mask

def overlay_mask_on_frame(frame, mask):
    """
    Ch·ªìng mask (ch·ªâ nh·ªØng pixel b·∫±ng 1) l√™n frame RGB.
    """
    mask_binary = (mask == 1).astype(np.uint8)
    color_mask = np.zeros_like(frame, dtype=np.uint8)
    color_mask[mask_binary == 1] = [0, 255, 0]  # M√†u xanh l√° cho crack

    alpha = 0.4
    overlaid = frame.copy()
    overlaid[mask_binary == 1] = (
        alpha * color_mask[mask_binary == 1] +
        (1 - alpha) * frame[mask_binary == 1]
    ).astype(np.uint8)

    return overlaid

def process_video(video_path, model, transform, device, output_path):
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("‚ùå Kh√¥ng th·ªÉ m·ªü video.")
        return

    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # D√πng codec XVID ƒë·ªÉ h·ªó tr·ª£ .avi
    fourcc = cv2.VideoWriter_fourcc(*'H264')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Chuy·ªÉn frame th√†nh PIL image ƒë·ªÉ d√πng transform
        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        mask = get_predicted_mask(model, pil_img, transform, device)

        # Resize l·∫°i mask v·ªÅ k√≠ch th∆∞·ªõc g·ªëc n·∫øu c·∫ßn (ƒë√£ x·ª≠ l√Ω trong get_predicted_mask)
        overlaid = overlay_mask_on_frame(frame, mask)

        out.write(overlaid)

        frame_idx += 1
        if frame_idx % 30 == 0:
            print(f"ƒê√£ x·ª≠ l√Ω {frame_idx} frame")

    out.release()
    cap.release()

    # time.sleep(1) 
    print(f"‚úÖ Video ƒë√£ l∆∞u t·∫°i: {output_path}")


import os
import cv2
import torch
from PIL import Image
import numpy as np
import streamlit as st

import cv2
import os
import time
from PIL import Image
import numpy as np

import cv2
import os
import time
import numpy as np
from PIL import Image

import io
import cv2
from PIL import Image
import numpy as np
import tempfile

def process_video_streamlit(video_path, model, transform, device):
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        st.error("‚ùå Kh√¥ng th·ªÉ m·ªü video.")
        return None

    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # T·∫°o video t·∫°m trong file ƒë·ªÉ ghi xong r·ªìi ƒë∆∞a v√†o BytesIO
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
        temp_video_path = tmp.name

    fourcc = cv2.VideoWriter_fourcc(*'H264')  # codec ph·ªï bi·∫øn
    out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Convert BGR ‚Üí RGB ‚Üí PIL ƒë·ªÉ d√πng model
        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        mask = get_predicted_mask(model, pil_img, transform, device)

        overlaid = overlay_mask_on_frame(frame, mask)
        overlaid = overlaid.astype(np.uint8)

        # Resize n·∫øu kh√°c k√≠ch th∆∞·ªõc g·ªëc
        if overlaid.shape[:2] != (height, width):
            overlaid = cv2.resize(overlaid, (width, height))

        out.write(overlaid)

    cap.release()
    out.release()

    # ƒê·ªçc l·∫°i video v·ª´a t·∫°o th√†nh BytesIO ƒë·ªÉ d√πng lu√¥n
    with open(temp_video_path, "rb") as f:
        video_bytes = f.read()

    video_io = io.BytesIO(video_bytes)
    video_io.seek(0)

    # Hi·ªÉn th·ªã video ngay
    st.video(temp_video_path)

    # N√∫t t·∫£i video
    st.download_button(
        label="üì• T·∫£i video k·∫øt qu·∫£",
        data=video_io.getvalue(),
        file_name="processed_video.mp4",
        mime="video/mp4"
    )

    return video_io  # C√≥ th·ªÉ d√πng ti·∫øp n·∫øu c·∫ßn



def process_video2(video_path, model, transform_img, device, output_path):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("‚ùå Kh√¥ng th·ªÉ m·ªü video.")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    def process_frame(image_pil):
        image_np = np.array(image_pil)
        raw_mask = get_predicted_mask(model, image_pil, transform_img, device)
        mask = merge_and_expand_mask(raw_mask, expand_px=10, connect_dist=100)

        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        mask_thuhai_aggregated = np.zeros(image_np.shape[:2], dtype=np.uint8)

        for cnt in cnts:
            if cv2.contourArea(cnt) < 100:
                continue

            cnts_expanded = expand_contour(cnt, expand_px=20)
            if len(cnts_expanded) == 0:
                continue
            cnt_exp = max(cnts_expanded, key=cv2.contourArea)

            mask_expanded = np.zeros(mask.shape, dtype=np.uint8)
            cv2.drawContours(mask_expanded, [cnt_exp], -1, 255, thickness=-1)

            cropped_img_masked, bbox = crop_image_by_mask(image_np, mask_expanded)
            if cropped_img_masked is None:
                continue

            cropped_img_pil = Image.fromarray(cropped_img_masked)
            mask_thuhai = get_predicted_mask(model, cropped_img_pil, transform_img, device)

            h_crop, w_crop = mask_thuhai.shape
            x_min, y_min, x_max, y_max = bbox

            mask_expanded_bin = (mask_expanded > 0).astype(np.uint8)
            mask_thuhai_bin = (mask_thuhai > 0).astype(np.uint8)

            mask_thuhai_filtered = mask_thuhai_bin * mask_expanded_bin[y_min:y_min + h_crop, x_min:x_min + w_crop]
            mask_thuhai_filtered = mask_thuhai_filtered * 255

            mask_thuhai_aggregated[y_min:y_min + h_crop, x_min:x_min + w_crop] = np.maximum(
                mask_thuhai_aggregated[y_min:y_min + h_crop, x_min:x_min + w_crop],
                mask_thuhai_filtered
            )

        return image_np, mask_thuhai_aggregated

    def overlay_mask_on_frame(frame_bgr, mask):
        # mask d·∫°ng 0 v√† 255, convert sang 3 channels m√†u xanh l√°
        green_mask = np.zeros_like(frame_bgr, dtype=np.uint8)
        green_mask[:, :, 1] = mask  # k√™nh G

        alpha = 0.5
        overlaid = cv2.addWeighted(frame_bgr, 1, green_mask, alpha, 0)
        return overlaid

    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        image_np, mask_aggregated = process_frame(pil_img)

        overlaid = overlay_mask_on_frame(frame, mask_aggregated)

        out.write(overlaid)

        frame_idx += 1
        if frame_idx % 30 == 0:
            print(f"ƒê√£ x·ª≠ l√Ω {frame_idx} frame")

    cap.release()
    out.release()
    print(f"‚úÖ Video ƒë√£ l∆∞u t·∫°i: {output_path}")

def expand_contour(cnt, expand_px=50):
    """
    M·ªü r·ªông v√πng contour b·∫±ng dilation tr√™n mask contour.
    """
    # T·∫°o mask ƒë·ªß l·ªõn ch·ª©a contour + kho·∫£ng m·ªü r·ªông
    h = cnt[:, 0, 1].max() + expand_px * 2
    w = cnt[:, 0, 0].max() + expand_px * 2
    h = max(h, 1024)
    w = max(w, 1024)
    mask = np.zeros((h, w), dtype=np.uint8)
    # V·∫Ω contour g·ªëc v·ªõi offset expand_px ƒë·ªÉ kh√¥ng b·ªã tr√†n
    cnt_offset = cnt + expand_px
    cv2.drawContours(mask, [cnt_offset], -1, 255, thickness=-1)
    # Dilation m·ªü r·ªông contour
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (expand_px * 2, expand_px * 2))
    mask_dilated = cv2.dilate(mask, kernel, iterations=1)
    # T√¨m contour m·ªõi tr√™n mask m·ªü r·ªông
    cnts_dilated, _ = cv2.findContours(mask_dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # D·ªãch l·∫°i contour v·ªÅ v·ªã tr√≠ ban ƒë·∫ßu (tr·ª´ ƒëi offset expand_px)
    cnts_expanded = [c - expand_px for c in cnts_dilated]
    return cnts_expanded

import numpy as np
import cv2

import cv2
import numpy as np

def crop_image_by_mask(image_np, mask, padding=5, min_size=1000, return_3ch=True):
    ys, xs = np.where(mask == 255)
    if len(xs) == 0 or len(ys) == 0:
        return None, None

    h, w = image_np.shape[:2]
    x_min = max(xs.min() - padding, 0)
    x_max = min(xs.max() + padding, w - 1)
    y_min = max(ys.min() - padding, 0)
    y_max = min(ys.max() + padding, h - 1)

    cur_w = x_max - x_min + 1
    cur_h = y_max - y_min + 1
    extra_w = max(min_size - cur_w, 0)
    extra_h = max(min_size - cur_h, 0)

    x_min = max(x_min - extra_w // 2, 0)
    x_max = min(x_max + (extra_w - extra_w // 2), w - 1)
    y_min = max(y_min - extra_h // 2, 0)
    y_max = min(y_max + (extra_h - extra_h // 2), h - 1)

    cropped_img = image_np[y_min:y_max+1, x_min:x_max+1].copy()
    cropped_mask = mask[y_min:y_max+1, x_min:x_max+1]

    # Chuy·ªÉn ·∫£nh sang kh√¥ng gian m√†u LAB
    lab = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)

    # √Åp d·ª•ng CLAHE ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô t∆∞∆°ng ph·∫£n
    clahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(8,8))
    l_clahe = clahe.apply(l)
    enhanced_lab = cv2.merge((l_clahe, a, b))
    enhanced_img = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)

    gray = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2GRAY)
    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')
    lbp_uint8 = np.uint8(lbp)  # Chuy·ªÉn ki·ªÉu v·ªÅ uint8 tr∆∞·ªõc khi threshold
    _, lbp_thresh = cv2.threshold(lbp_uint8, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    lbp_mask = cv2.bitwise_and(cropped_mask, lbp_thresh)

    # T·∫°o ·∫£nh n·ªÅn m·ªù
    blurred_img = cv2.GaussianBlur(cropped_img, (21, 21), sigmaX=10, sigmaY=10)

    # K·∫øt h·ª£p ·∫£nh n·ªÅn m·ªù v√† ·∫£nh v·∫øt n·ª©t ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán
    result = np.where(lbp_mask[:, :, None] == 255, enhanced_img, blurred_img)

    # L√†m m∆∞·ª£t k·∫øt qu·∫£
    result_smooth = cv2.bilateralFilter(result, d=15, sigmaColor=150, sigmaSpace=150)

    # Chuy·ªÉn ·∫£nh v·ªÅ grayscale
    result_gray = cv2.cvtColor(result_smooth, cv2.COLOR_RGB2GRAY)

    if return_3ch:
        result_out = np.stack([result_gray]*3, axis=-1)
    else:
        result_out = result_gray

    return result_out.astype(np.uint8), (x_min, y_min, x_max, y_max)
